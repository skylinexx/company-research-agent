[![en](https://img.shields.io/badge/lang-en-red.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.md)
[![zh](https://img.shields.io/badge/lang-zh-green.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.zh.md)
[![fr](https://img.shields.io/badge/lang-fr-blue.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.fr.md)
[![es](https://img.shields.io/badge/lang-es-yellow.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.es.md)


# Agent de Recherche d'Entreprise üîç

![web ui](<static/ui-1.png>)

Un outil multi-agents qui g√©n√®re des rapports de recherche d'entreprise complets. La plateforme utilise un pipeline d'agents IA pour collecter, organiser et synth√©tiser des informations sur n'importe quelle entreprise.

‚ú®Essayez-le en ligne ! https://companyresearcher.tavily.com ‚ú®

https://github.com/user-attachments/assets/0e373146-26a7-4391-b973-224ded3182a9

## Fonctionnalit√©s

- **Recherche Multi-Sources** : R√©cup√®re des donn√©es de diverses sources, y compris les sites web d'entreprise, articles de presse, rapports financiers et analyses sectorielles
- **Filtrage de contenu par IA** : Utilise le score de pertinence de Tavily pour la curation du contenu
- **Streaming en temps r√©el** : Utilise les WebSockets pour diffuser l'avancement et les r√©sultats de la recherche en temps r√©el
- **Architecture √† double mod√®le** :
  - Gemini 2.0 Flash pour la synth√®se de recherche √† large contexte
  - GPT-4.1 pour la mise en forme et l'√©dition pr√©cises du rapport
- **Frontend React moderne** : Interface r√©active avec mises √† jour en temps r√©el, suivi de progression et options de t√©l√©chargement
- **Architecture modulaire** : Construite autour d'un pipeline de n≈ìuds sp√©cialis√©s de recherche et de traitement

## Cadre Agentique

### Pipeline de Recherche

La plateforme suit un cadre agentique avec des n≈ìuds sp√©cialis√©s qui traitent les donn√©es de mani√®re s√©quentielle :

1. **N≈ìuds de Recherche** :
   - `CompanyAnalyzer` : Recherche les informations principales sur l'entreprise
   - `IndustryAnalyzer` : Analyse la position sur le march√© et les tendances
   - `FinancialAnalyst` : R√©cup√®re les indicateurs financiers et les donn√©es de performance
   - `NewsScanner` : Collecte les actualit√©s et d√©veloppements r√©cents

2. **N≈ìuds de Traitement** :
   - `Collector` : Agr√®ge les donn√©es de recherche de tous les analyseurs
   - `Curator` : Met en ≈ìuvre le filtrage de contenu et le scoring de pertinence
   - `Briefing` : G√©n√®re des synth√®ses par cat√©gorie √† l'aide de Gemini 2.0 Flash
   - `Editor` : Compile et met en forme les synth√®ses dans un rapport final avec GPT-4.1-mini

   ![web ui](<static/agent-flow.png>)

### Architecture de G√©n√©ration de Contenu

La plateforme exploite des mod√®les distincts pour des performances optimales :

1. **Gemini 2.0 Flash** (`briefing.py`) :
   - G√®re la synth√®se de recherche √† large contexte
   - Excelle dans le traitement et le r√©sum√© de grands volumes de donn√©es
   - Utilis√© pour g√©n√©rer les synth√®ses initiales par cat√©gorie
   - Efficace pour maintenir le contexte sur plusieurs documents

2. **GPT-4.1 mini** (`editor.py`) :
   - Sp√©cialis√© dans la mise en forme et l'√©dition pr√©cises
   - G√®re la structure markdown et la coh√©rence
   - Sup√©rieur pour suivre des instructions de formatage exactes
   - Utilis√© pour :
     - Compilation du rapport final
     - D√©duplication du contenu
     - Mise en forme markdown
     - Streaming du rapport en temps r√©el

Cette approche combine la capacit√© de Gemini √† g√©rer de larges fen√™tres de contexte avec la pr√©cision de GPT-4.1-mini pour le respect des consignes de formatage.

### Syst√®me de Curation de Contenu

La plateforme utilise un syst√®me de filtrage de contenu dans `curator.py` :

1. **Scoring de Pertinence** :
   - Les documents sont scor√©s par la recherche IA de Tavily
   - Un seuil minimum (par d√©faut 0,4) est requis pour continuer
   - Les scores refl√®tent la pertinence par rapport √† la requ√™te de recherche
   - Un score √©lev√© indique une meilleure correspondance avec l'intention de recherche

2. **Traitement des Documents** :
   - Le contenu est normalis√© et nettoy√©
   - Les URLs sont d√©dupliqu√©es et standardis√©es
   - Les documents sont tri√©s par score de pertinence
   - Les mises √† jour de progression sont envoy√©es en temps r√©el via WebSocket

### Syst√®me de Communication en Temps R√©el

La plateforme impl√©mente un syst√®me de communication en temps r√©el bas√© sur WebSocket :

![web ui](<static/ui-2.png>)

1. **Impl√©mentation Backend** :
   - Utilise le support WebSocket de FastAPI
   - Maintient des connexions persistantes par t√¢che de recherche
   - Envoie des mises √† jour structur√©es pour divers √©v√©nements :
     ```python
     await websocket_manager.send_status_update(
         job_id=job_id,
         status="processing",
         message=f"G√©n√©ration du briefing {category}",
         result={
             "step": "Briefing",
             "category": category,
             "total_docs": len(docs)
         }
     )
     ```

2. **Int√©gration Frontend** :
   - Les composants React s'abonnent aux mises √† jour WebSocket
   - Les mises √† jour sont trait√©es et affich√©es en temps r√©el
   - Diff√©rents composants UI g√®rent des types d'updates sp√©cifiques :
     - Progression de la g√©n√©ration de requ√™te
     - Statistiques de curation de documents
     - Statut de compl√©tion des briefings
     - Progression de la g√©n√©ration du rapport

3. **Types de Statut** :
   - `query_generating` : Mises √† jour de cr√©ation de requ√™te en temps r√©el
   - `document_kept` : Progression de la curation de documents
   - `briefing_start/complete` : Statut de g√©n√©ration des briefings
   - `report_chunk` : Streaming de la g√©n√©ration du rapport
   - `curation_complete` : Statistiques finales des documents

## Configuration

### Configuration Rapide (Recommand√©e)

La fa√ßon la plus simple de commencer est d'utiliser le script de configuration :

1. Clonez le d√©p√¥t :
```bash
git clone https://github.com/pogjester/tavily-company-research.git
cd tavily-company-research
```

2. Rendez le script de configuration ex√©cutable et lancez-le :
```bash
chmod +x setup.sh
./setup.sh
```

Le script de configuration va :
- V√©rifier les versions requises de Python et Node.js
- Cr√©er √©ventuellement un environnement virtuel Python (recommand√©)
- Installer toutes les d√©pendances (Python et Node.js)
- Vous guider dans la configuration de vos variables d'environnement
- D√©marrer √©ventuellement les serveurs backend et frontend

Vous aurez besoin des cl√©s API suivantes :
- Cl√© API Tavily
- Cl√© API Google Gemini
- Cl√© API OpenAI
- URI MongoDB (optionnel)

### Configuration Manuelle

Si vous pr√©f√©rez configurer manuellement, suivez ces √©tapes :

1. Clonez le d√©p√¥t :
```bash
git clone https://github.com/pogjester/tavily-company-research.git
cd tavily-company-research
```

2. Installez les d√©pendances backend :
```bash
# Optionnel : Cr√©ez et activez un environnement virtuel
python -m venv .venv
source .venv/bin/activate

# Installez les d√©pendances Python
pip install -r requirements.txt
```

3. Installez les d√©pendances frontend :
```bash
cd ui
npm install
```

4. Cr√©ez un fichier `.env` avec vos cl√©s API :
```env
TAVILY_API_KEY=votre_cl√©_tavily
GEMINI_API_KEY=votre_cl√©_gemini
OPENAI_API_KEY=votre_cl√©_openai

# Optionnel : Activez la persistance MongoDB
# MONGODB_URI=votre_cha√Æne_de_connexion_mongodb
```

### Configuration Docker

L'application peut √™tre ex√©cut√©e √† l'aide de Docker et Docker Compose :

1. Clonez le d√©p√¥t :
```bash
git clone https://github.com/pogjester/tavily-company-research.git
cd tavily-company-research
```

2. Cr√©ez un fichier `.env` avec vos cl√©s API :
```env
TAVILY_API_KEY=votre_cl√©_tavily
GEMINI_API_KEY=votre_cl√©_gemini
OPENAI_API_KEY=votre_cl√©_openai

# Optionnel : Activez la persistance MongoDB
# MONGODB_URI=votre_cha√Æne_de_connexion_mongodb
```

3. Construisez et d√©marrez les conteneurs :
```bash
docker compose up --build
```

Cela d√©marrera les services backend et frontend :
- L'API backend sera disponible sur `http://localhost:8000`
- Le frontend sera disponible sur `http://localhost:5174`

Pour arr√™ter les services :
```bash
docker compose down
```

Remarque : Lors de la mise √† jour des variables d'environnement dans `.env`, vous devrez red√©marrer les conteneurs :
```bash
docker compose down && docker compose up
```

### Ex√©cution de l'Application

1. D√©marrez le serveur backend (choisissez une option) :
```bash
# Option 1 : Module Python Direct
python -m application.py

# Option 2 : FastAPI avec Uvicorn
uvicorn application:app --reload --port 8000
```

2. Dans un nouveau terminal, d√©marrez le frontend :
```bash
cd ui
npm run dev
```

3. Acc√©dez √† l'application sur `http://localhost:5173`

## Utilisation

### D√©veloppement Local

1. D√©marrez le serveur backend (choisissez une option) :

   **Option 1 : Module Python Direct**
   ```bash
   python -m application.py
   ```

   **Option 2 : FastAPI avec Uvicorn**
   ```bash
   # Installez uvicorn si ce n'est pas d√©j√† fait
   pip install uvicorn

   # Ex√©cutez l'application FastAPI avec rechargement √† chaud
   uvicorn application:app --reload --port 8000
   ```

   Le backend sera disponible sur :
   - Point d'acc√®s API : `http://localhost:8000`
   - Point d'acc√®s WebSocket : `ws://localhost:8000/research/ws/{job_id}`

2. D√©marrez le serveur de d√©veloppement frontend :
   ```bash
   cd ui
   npm run dev
   ```

3. Acc√©dez √† l'application sur `http://localhost:5173`

### Options de D√©ploiement

L'application peut √™tre d√©ploy√©e sur diverses plateformes cloud. Voici quelques options courantes :

#### AWS Elastic Beanstalk

1. Installez l'EB CLI :
   ```bash
   pip install awsebcli
   ```

2. Initialisez l'application EB :
   ```bash
   eb init -p python-3.11 tavily-research
   ```

3. Cr√©ez et d√©ployez :
   ```bash
   eb create tavily-research-prod
   ```

#### Autres Options de D√©ploiement

- **Docker** : L'application inclut un Dockerfile pour le d√©ploiement conteneuris√©
- **Heroku** : D√©ployez directement depuis GitHub avec le buildpack Python
- **Google Cloud Run** : Adapt√© au d√©ploiement conteneuris√© avec mise √† l'√©chelle automatique

Choisissez la plateforme qui convient le mieux √† vos besoins. L'application est ind√©pendante de la plateforme et peut √™tre h√©berg√©e partout o√π les applications web Python sont prises en charge.

## Contribution

1. Forkez le d√©p√¥t
2. Cr√©ez une branche de fonctionnalit√© (`git checkout -b fonctionnalite/superbe-fonction`)
3. Validez vos modifications (`git commit -m 'Ajout d'une superbe fonction'`)
4. Poussez vers la branche (`git push origin fonctionnalite/superbe-fonction`)
5. Ouvrez une Pull Request

## Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## Remerciements

- [Tavily](https://tavily.com/) pour l'API de recherche
- Toutes les autres biblioth√®ques open-source et leurs contributeurs
