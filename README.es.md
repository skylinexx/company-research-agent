[![en](https://img.shields.io/badge/lang-en-red.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.md)
[![zh](https://img.shields.io/badge/lang-zh-green.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.zh.md)
[![fr](https://img.shields.io/badge/lang-fr-blue.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.fr.md)
[![es](https://img.shields.io/badge/lang-es-yellow.svg)](https://github.com/pogjester/company-research-agent/blob/main/README.es.md)

# Investigador de Empresas üîç

![interfaz web](<static/ui-1.png>)

Una herramienta multi-agente que genera informes de investigaci√≥n exhaustivos sobre empresas. La plataforma utiliza un sistema de agentes de IA para recopilar, seleccionar y sintetizar informaci√≥n sobre cualquier empresa.

‚ú®¬°Pru√©balo en l√≠nea! https://companyresearcher.tavily.com ‚ú®

https://github.com/user-attachments/assets/0e373146-26a7-4391-b973-224ded3182a9

## Caracter√≠sticas

- **Investigaci√≥n Multi-Fuente**: Recopila datos de diversas fuentes, incluyendo sitios web de empresas, art√≠culos de noticias, informes financieros y an√°lisis sectoriales
- **Filtrado de Contenido Impulsado por IA**: Utiliza la puntuaci√≥n de relevancia de Tavily para la selecci√≥n de contenido
- **Transmisi√≥n de Progreso en Tiempo Real**: Utiliza conexiones WebSocket para transmitir el progreso de la investigaci√≥n y los resultados
- **Arquitectura de Modelo Dual**: 
  - Gemini 2.0 Flash para s√≠ntesis de investigaci√≥n de alto contexto
  - GPT-4.1 para formato preciso y edici√≥n de informes
- **Frontend Moderno en React**: Interfaz de usuario receptiva con actualizaciones en tiempo real, seguimiento de progreso y opciones de descarga
- **Arquitectura Modular**: Construido utilizando un sistema de nodos de investigaci√≥n y procesamiento especializados

## Marco de Agentes

### Sistema de Investigaci√≥n

La plataforma sigue un marco basado en agentes con nodos especializados que procesan datos secuencialmente:

1. **Nodos de Investigaci√≥n**:
   - `CompanyAnalyzer`: Investiga informaci√≥n b√°sica del negocio
   - `IndustryAnalyzer`: Analiza posici√≥n de mercado y tendencias
   - `FinancialAnalyst`: Recopila m√©tricas financieras y datos de rendimiento
   - `NewsScanner`: Recopila noticias y desarrollos recientes

2. **Nodos de Procesamiento**:
   - `Collector`: Agrega datos de investigaci√≥n de todos los analizadores
   - `Curator`: Implementa filtrado de contenido y puntuaci√≥n de relevancia
   - `Briefing`: Genera res√∫menes espec√≠ficos por categor√≠a utilizando Gemini 2.0 Flash
   - `Editor`: Compila y formatea los res√∫menes en un informe final utilizando GPT-4.1-mini

   ![interfaz web](<static/agent-flow.png>)

### Arquitectura de Generaci√≥n de Contenido

La plataforma aprovecha modelos separados para un rendimiento √≥ptimo:

1. **Gemini 2.0 Flash** (`briefing.py`):
   - Maneja tareas de s√≠ntesis de investigaci√≥n de alto contexto
   - Sobresale en el procesamiento y resumen de grandes vol√∫menes de datos
   - Utilizado para generar res√∫menes iniciales por categor√≠a
   - Eficiente en mantener el contexto a trav√©s de m√∫ltiples documentos

2. **GPT-4.1 mini** (`editor.py`):
   - Se especializa en tareas precisas de formato y edici√≥n
   - Maneja la estructura y consistencia en markdown
   - Superior en seguir instrucciones exactas de formato
   - Utilizado para:
     - Compilaci√≥n final del informe
     - Eliminaci√≥n de duplicados de contenido
     - Formateo en markdown
     - Transmisi√≥n de informes en tiempo real

Este enfoque combina la fortaleza de Gemini en el manejo de ventanas de contexto grandes con la precisi√≥n de GPT-4.1-mini en seguir instrucciones espec√≠ficas de formato.

### Sistema de Selecci√≥n de Contenido

La plataforma utiliza un sistema de filtrado de contenido en `curator.py`:

1. **Puntuaci√≥n de Relevancia**:
   - Los documentos son puntuados por la b√∫squeda potenciada por IA de Tavily
   - Se requiere un umbral m√≠nimo (predeterminado 0.4) para proceder
   - Las puntuaciones reflejan la relevancia para la consulta de investigaci√≥n espec√≠fica
   - Puntuaciones m√°s altas indican mejores coincidencias con la intenci√≥n de la investigaci√≥n

2. **Procesamiento de Documentos**:
   - El contenido se normaliza y limpia
   - Las URLs se desduplicaron y estandarizaron
   - Los documentos se ordenan por puntuaciones de relevancia
   - Las actualizaciones de progreso en tiempo real se env√≠an a trav√©s de WebSocket

### Sistema de Comunicaci√≥n en Tiempo Real

La plataforma implementa un sistema de comunicaci√≥n en tiempo real basado en WebSocket:

![interfaz web](<static/ui-2.png>)

1. **Implementaci√≥n Backend**:
   - Utiliza el soporte de WebSocket de FastAPI
   - Mantiene conexiones persistentes por trabajo de investigaci√≥n
   - Env√≠a actualizaciones de estado estructuradas para varios eventos:
     ```python
     await websocket_manager.send_status_update(
         job_id=job_id,
         status="processing",
         message=f"Generating {category} briefing",
         result={
             "step": "Briefing",
             "category": category,
             "total_docs": len(docs)
         }
     )
     ```

2. **Integraci√≥n Frontend**:
   - Los componentes de React se suscriben a actualizaciones WebSocket
   - Las actualizaciones se procesan y muestran en tiempo real
   - Diferentes componentes de UI manejan tipos espec√≠ficos de actualizaciones:
     - Progreso de generaci√≥n de consultas
     - Estad√≠sticas de selecci√≥n de documentos
     - Estado de finalizaci√≥n de res√∫menes
     - Progreso de generaci√≥n de informes

3. **Tipos de Estado**:
   - `query_generating`: Actualizaciones en tiempo real de creaci√≥n de consultas
   - `document_kept`: Progreso de selecci√≥n de documentos
   - `briefing_start/complete`: Estado de generaci√≥n de res√∫menes
   - `report_chunk`: Transmisi√≥n de generaci√≥n de informes
   - `curation_complete`: Estad√≠sticas finales de documentos

## Instalaci√≥n

### Instalaci√≥n R√°pida (Recomendada)

La forma m√°s sencilla de comenzar es utilizando el script de instalaci√≥n:

1. Clonar el repositorio:
```bash
git clone https://github.com/pogjester/tavily-company-research.git
cd tavily-company-research
```

2. Hacer que el script de instalaci√≥n sea ejecutable y ejecutarlo:
```bash
chmod +x setup.sh
./setup.sh
```

El script de instalaci√≥n har√° lo siguiente:
- Verificar las versiones requeridas de Python y Node.js
- Opcionalmente crear un entorno virtual de Python (recomendado)
- Instalar todas las dependencias (Python y Node.js)
- Guiarte a trav√©s de la configuraci√≥n de tus variables de entorno
- Opcionalmente iniciar los servidores de backend y frontend

Necesitar√°s tener listas las siguientes claves API:
- Clave API de Tavily
- Clave API de Google Gemini
- Clave API de OpenAI
- URI de MongoDB (opcional)

### Instalaci√≥n Manual

Si prefieres realizar la instalaci√≥n manualmente, sigue estos pasos:

1. Clonar el repositorio:
```bash
git clone https://github.com/pogjester/tavily-company-research.git
cd tavily-company-research
```

2. Instalar dependencias de backend:
```bash
# Opcional: Crear y activar entorno virtual
python -m venv .venv
source .venv/bin/activate

# Instalar dependencias de Python
pip install -r requirements.txt
```

3. Instalar dependencias de frontend:
```bash
cd ui
npm install
```

4. Crear un archivo `.env` con tus claves API:
```env
TAVILY_API_KEY=tu_clave_tavily
GEMINI_API_KEY=tu_clave_gemini
OPENAI_API_KEY=tu_clave_openai

# Opcional: Habilitar persistencia en MongoDB
# MONGODB_URI=tu_cadena_de_conexion_mongodb
```

### Instalaci√≥n con Docker

La aplicaci√≥n puede ejecutarse utilizando Docker y Docker Compose:

1. Clonar el repositorio:
```bash
git clone https://github.com/pogjester/tavily-company-research.git
cd tavily-company-research
```

2. Crear un archivo `.env` con tus claves API:
```env
TAVILY_API_KEY=tu_clave_tavily
GEMINI_API_KEY=tu_clave_gemini
OPENAI_API_KEY=tu_clave_openai

# Opcional: Habilitar persistencia en MongoDB
# MONGODB_URI=tu_cadena_de_conexion_mongodb
```

3. Construir e iniciar los contenedores:
```bash
docker compose up --build
```

Esto iniciar√° los servicios de backend y frontend:
- La API de backend estar√° disponible en `http://localhost:8000`
- El frontend estar√° disponible en `http://localhost:5174`

Para detener los servicios:
```bash
docker compose down
```

Nota: Al actualizar las variables de entorno en `.env`, necesitar√°s reiniciar los contenedores:
```bash
docker compose down && docker compose up
```

### Ejecutando la Aplicaci√≥n

1. Iniciar el servidor de backend (elige una opci√≥n):
```bash
# Opci√≥n 1: M√≥dulo Python Directo
python -m application.py

# Opci√≥n 2: FastAPI con Uvicorn
uvicorn application:app --reload --port 8000
```

2. En una nueva terminal, iniciar el frontend:
```bash
cd ui
npm run dev
```

3. Acceder a la aplicaci√≥n en `http://localhost:5173`

## Uso

### Desarrollo Local

1. Iniciar el servidor de backend (elige una opci√≥n):

   **Opci√≥n 1: M√≥dulo Python Directo**
   ```bash
   python -m application.py
   ```

   **Opci√≥n 2: FastAPI con Uvicorn**
   ```bash
   # Instalar uvicorn si a√∫n no est√° instalado
   pip install uvicorn

   # Ejecutar la aplicaci√≥n FastAPI con recarga autom√°tica
   uvicorn application:app --reload --port 8000
   ```

   El backend estar√° disponible en:
   - Punto de conexi√≥n API: `http://localhost:8000`
   - Punto de conexi√≥n WebSocket: `ws://localhost:8000/research/ws/{job_id}`

2. Iniciar el servidor de desarrollo del frontend:
   ```bash
   cd ui
   npm run dev
   ```

3. Acceder a la aplicaci√≥n en `http://localhost:5173`

### Opciones de Despliegue

La aplicaci√≥n puede desplegarse en varias plataformas en la nube. Aqu√≠ hay algunas opciones comunes:

#### AWS Elastic Beanstalk

1. Instalar el EB CLI:
   ```bash
   pip install awsebcli
   ```

2. Inicializar la aplicaci√≥n EB:
   ```bash
   eb init -p python-3.11 tavily-research
   ```

3. Crear y desplegar:
   ```bash
   eb create tavily-research-prod
   ```

#### Otras Opciones de Despliegue

- **Docker**: La aplicaci√≥n incluye un Dockerfile para despliegue en contenedores
- **Heroku**: Despliegue directamente desde GitHub con el buildpack de Python
- **Google Cloud Run**: Adecuado para despliegue en contenedores con escalado autom√°tico

Elige la plataforma que mejor se adapte a tus necesidades. La aplicaci√≥n es independiente de la plataforma y puede alojarse en cualquier lugar que admita aplicaciones web Python.

## Contribuir

1. Haz un fork del repositorio
2. Crea una rama de caracter√≠sticas (`git checkout -b feature/caracteristica-increible`)
3. Haz commit de tus cambios (`git commit -m 'A√±adir caracter√≠stica incre√≠ble'`)
4. Haz push a la rama (`git push origin feature/caracteristica-increible`)
5. Abre un Pull Request

## Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - consulta el archivo [LICENSE](LICENSE) para m√°s detalles.

## Agradecimientos

- [Tavily](https://tavily.com/) por la API de investigaci√≥n
- Todas las dem√°s bibliotecas de c√≥digo abierto y sus contribuyentes
